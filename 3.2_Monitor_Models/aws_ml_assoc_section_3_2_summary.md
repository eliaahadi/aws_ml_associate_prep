# AWS ML Associate --- Section 3.2: Monitor Models

This section focuses on monitoring deployed models for performance, data
drift, and bias. It highlights SageMaker monitoring services,
integration with CloudWatch, and responsible model tracking.

# Monitoring Tools

• SageMaker Model Monitor --- detects data drift, anomalies, and quality
issues in production

• CloudWatch Metrics/Logs --- captures performance, latency, and
endpoint health

• SageMaker Clarify --- monitors bias and explains predictions
(training + inference)

# Exam Tips

• Model Monitor = main tool for drift and anomalies

• Clarify = fairness and explainability in both training and inference

• CloudWatch = operational monitoring (metrics + logs)

• Always set alerts for model drift or data quality issues

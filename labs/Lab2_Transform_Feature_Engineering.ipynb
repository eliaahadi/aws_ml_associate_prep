{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029da686",
   "metadata": {},
   "source": [
    "# Lab 2 â€” Transform & Feature Engineering (AWS ML Associate)\n",
    "Use **SageMaker Processing (sklearn)** to clean/encode/scale and split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d439dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAB_BUCKET = 's3://CHANGE-ME-BUCKET'\n",
    "LAB_PREFIX = 'ml-assoc/l2'\n",
    "print(LAB_BUCKET, LAB_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8e577",
   "metadata": {},
   "source": [
    "### Upload input CSV to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757069c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp /tmp/l1/customers.csv {LAB_BUCKET}/{LAB_PREFIX}/input/customers.csv || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d60641",
   "metadata": {},
   "source": [
    "### Create processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > /tmp/processing_l2.py << 'PY'\n",
    "import argparse, os\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "def main():\n",
    "  p=argparse.ArgumentParser(); p.add_argument('--input'); p.add_argument('--output'); a=p.parse_args()\n",
    "  df=pd.read_csv(os.path.join(a.input,'customers.csv')).dropna()\n",
    "  X=df[['age','income','state']]; y=df['churn']\n",
    "  X_cat=OneHotEncoder(sparse_output=False,handle_unknown='ignore').fit_transform(X[['state']])\n",
    "  X_num=StandardScaler().fit_transform(X[['age','income']])\n",
    "  import numpy as np; Xp=np.hstack([X_num,X_cat])\n",
    "  Xtr,Xte,ytr,yte=train_test_split(Xp,y,test_size=0.2,stratify=y,random_state=42)\n",
    "  os.makedirs(a.output,exist_ok=True)\n",
    "  pd.DataFrame(Xtr).to_csv(os.path.join(a.output,'X_train.csv'),index=False)\n",
    "  pd.DataFrame(Xte).to_csv(os.path.join(a.output,'X_test.csv'),index=False)\n",
    "  ytr.to_csv(os.path.join(a.output,'y_train.csv'),index=False)\n",
    "  yte.to_csv(os.path.join(a.output,'y_test.csv'),index=False)\n",
    "if __name__=='__main__': main()\n",
    "PY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9952d8f",
   "metadata": {},
   "source": [
    "### Run Processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, os\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "sess=sagemaker.Session(); role=sagemaker.get_execution_role(); bucket=LAB_BUCKET.replace('s3://','')\n",
    "proc=SKLearnProcessor(framework_version='1.2-1', role=role, instance_type='ml.m5.large', instance_count=1)\n",
    "proc.run(code='/tmp/processing_l2.py',\n",
    "        inputs=[ProcessingInput(source=f's3://{bucket}/{LAB_PREFIX}/input/', destination='/opt/ml/processing/input')],\n",
    "        outputs=[ProcessingOutput(source='/opt/ml/processing/output', destination=f's3://{bucket}/{LAB_PREFIX}/output/')],\n",
    "        arguments=['--input','/opt/ml/processing/input','--output','/opt/ml/processing/output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f074c",
   "metadata": {},
   "source": [
    "### Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d070c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {LAB_BUCKET}/{LAB_PREFIX}/output/"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
